{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bbf5cde8-c299-4f87-ac87-4a420b0bf611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6c05c702-5ec9-4272-866f-370b99f18531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker \n",
    "from sagemaker import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "29765664-ab44-4caa-9596-cdb8bb433501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "bucket = Session().default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1dcceb1f-7d0c-4189-a1a8-204224788964",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_id = \"pytorch-training-2023-12-16-13-20-59-388\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "40acb56f-3d86-4ed6-99e3-e74b789a880a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-16 13:40:03   40131847 model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://{bucket}/{training_job_id}/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1ffc8b04-55bf-4c0a-94b3-f1b498b68402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./model/model.tar.gz']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.s3.S3Downloader().download(\n",
    "    f's3://{bucket}/{training_job_id}/output/model.tar.gz',\n",
    "    local_path=\"./\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "191feec4-3a5f-4d5c-90c0-b889e42539a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "nanoGPT.pth\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf model.tar.gz --directory ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7b683538-3719-40ec-9b2d-7b6d8a4de78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc81d365710>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64  # how many independent sequences will we process in parallel\n",
    "block_size = 256  # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "eval_iters = 200\n",
    "#\n",
    "n_embed = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "#\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "65cc81d6-d8b6-420e-a724-58b55d45ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B, T, C)\n",
    "        q = self.query(x)  # (B, T, C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.05  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B, T, C)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head) -> None:\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class BigramLanguageMmodel(nn.Module):\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_talbe = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embed, n_head=n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embed)  # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos_emb = self.position_embedding_talbe(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        x = self.blocks(x)  # (B, T, C)\n",
    "        x = self.ln_f(x)  # (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            # softmax and loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indicies in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop ind to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # become (B, C)\n",
    "            # apply softmax to get probablities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ed039104-3820-42f0-84cd-e801fb799ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageMmodel(vocab_size=65)\n",
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e01715ee-8243-46da-8fb3-516122f2cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./model/nanoGPT.pth\", \"rb\") as f:\n",
    "    model.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6ce55b05-480b-47fd-8045-c1c5962f5af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bda77f39-c774-4390-80d9-878c81958697",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5e76fde8-7ee9-4bab-85e3-2fdaad7028e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\", 'r', encoding='utf-8') as f:\n",
    "  text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "02763be6-266d-4afe-a4a4-e2e3b545247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size =len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c8a20e43-cafd-44e0-9163-874db107a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars)}\n",
    "itos = { i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encode take a string and output list of integer\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decode a list of integer to a string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ba7573c7-0e2b-4988-84e2-a0013fa43a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Menenius, I must thou contrady?\n",
      "Is not thyself and full of this act thou suck'st thy wit.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Assist me lord\n",
      "Is all thine injurious bade.\n",
      "\n",
      "KING RICHARD III:\n",
      "Why, sir? is thy soul tumbles of me,\n",
      "And so this Claudio strike, is deceived.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "My gracious lord, so let's self--\n",
      "\n",
      "KING RICHARD III:\n",
      "I will not subdue think\n",
      "My throne not with tyrant thee in itself.\n",
      "What the fury of protectors be that thy business\n",
      "Most kindred thy gentleman law that it,\n",
      "Would have more than thee\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.module.generate(idx=torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bb91cd-3ed6-4d37-9c55-de27621196c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
